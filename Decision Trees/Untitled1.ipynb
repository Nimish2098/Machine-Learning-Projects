{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a17df10-b357-449b-9a86-2344bd91a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3507697c-0653-43e5-8a7f-4e03b9488707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate entropy\n",
    "def entropy(y):\n",
    "    unique_classes, counts = np.unique(y,return_count=True)\n",
    "    probabilities = counts/len(y)\n",
    "    entropy_value = -np.sum([p*np.log2(p) for p in probabilites])\n",
    "    return entropy_value\n",
    "\n",
    "# calculate the information gain\n",
    "def information_gain(x_column,y_column):\n",
    "    parent_entropy = entropy(y)\n",
    "\n",
    "    left_idx = x_column<=threshold\n",
    "    right_idx = x_column>threshold\n",
    "    n,n_left, n_right = len(y),np.sum(left_idx),np.sum(right_idx)\n",
    "\n",
    "    if n_left == 0 or n__right == 0:\n",
    "        return 0\n",
    "    left_entropy = entropy(y[left_idx])\n",
    "    right_entropy = entropy(y[right_idx])\n",
    "\n",
    "    weighted_avg_entropy = (n_left/n)* left_entropy + (n_right/n) * right_entropy\n",
    "    ig = parent_entropy - weighted_avg_entropy\n",
    "    return ig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2e8a243-5547-4e24-9ca9-53ac244dbe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the decision node\n",
    "class DecisionNode:\n",
    "    def __init__(self,feature=None,threshold = None, left = None, right=None, value = None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "# define the decision tree\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(selft,min_samples_splits = 2,max_depth = 100):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.root =  None\n",
    "\n",
    "    def fit(self,x,y):\n",
    "        self.root = self.grow_tree(x,y)\n",
    "\n",
    "    def _grow_tree(self,x,y,depth=0):\n",
    "        n_samples , n_features = x.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        if(depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split):\n",
    "            leaf_value = self.most_common_label(y)\n",
    "            return DecisionNode(value=leaf_value)\n",
    "\n",
    "        best_feature, best_threshold = self.best_split(x,y,n_features)\n",
    "    \n",
    "        left_idx = x[:,best_feature]<= best_threshold\n",
    "        right_idx = x[:, best_feature]>best_threshold\n",
    "\n",
    "        left_child = self.grow_tree(x[left_idx,:],y[left_idx],depth+1)\n",
    "        right_child = self.grow_tree(x[right_idx,:],y[right_idx],depth+1)\n",
    "        return DecisionNode(best_feature,best_threshold,left_child,right_child)\n",
    "\n",
    "    def _best_split(self,x,y,n_features):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None,None\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            x_colunn = x[:,feature_idx]\n",
    "            threshold = np.unique(x_column)\n",
    "            for threshold in thresholds:\n",
    "                ig = information_gain(x_column,y,threshold)\n",
    "                if ig>best_gain:\n",
    "                    best_gain = ig\n",
    "                    split_idx = feature_idx\n",
    "                    split_threshold = threshold\n",
    "        return split_idx,split_threshold\n",
    "    def _most_common_label(self,y):\n",
    "        unique_classes, counts = np.unique(y,return_counts = True)\n",
    "        most_common = unique_classes[np.argmax(counts)]\n",
    "        return most_common\n",
    "    def predict(self,x):\n",
    "        return np.array([self._traverse_tree(x,self.root) for x in x])\n",
    "\n",
    "    def _traverse_tree(self,x_node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature]<=node.threshold:\n",
    "            return self.traverse_tree(x,node.left)\n",
    "        else:\n",
    "            return self.traverse_tree(x,node.right)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8923f63b-40a9-4650-a8c2-96a5167849d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'min_samples_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m x \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature1\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature2\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      9\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m---> 11\u001b[0m tree \u001b[38;5;241m=\u001b[39m \u001b[43mDecisionTreeClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m tree\u001b[38;5;241m.\u001b[39mfit(x,y)\n\u001b[0;32m     14\u001b[0m prediction \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mpredict(x)\n",
      "Cell \u001b[1;32mIn[21], line 13\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.__init__\u001b[1;34m(selft, min_samples_splits, max_depth)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(selft,min_samples_splits \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m,max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_split \u001b[38;5;241m=\u001b[39m \u001b[43mmin_samples_split\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth \u001b[38;5;241m=\u001b[39m max_depth\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'min_samples_split' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    data = {\n",
    "        'feature1':[1,2,3,4,5],\n",
    "        'feature2':[1,0,1,0,1],\n",
    "        'label':[0,0,1,1,1]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    x = df[['feature1','feature2']].values\n",
    "    y = df['label'].values\n",
    "\n",
    "    tree = DecisionTreeClassifier(max_depth=3)\n",
    "    tree.fit(x,y)\n",
    "\n",
    "    prediction = tree.predict(x)\n",
    "\n",
    "    print(\"Predictions: \",predictions)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
